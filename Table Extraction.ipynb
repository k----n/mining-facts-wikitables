{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from wikitables import *\n",
    "import mwclient\n",
    "import mwparserfromhell as mwp\n",
    "import logging\n",
    "from collections import defaultdict\n",
    "nested_dict = lambda: defaultdict(nested_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "logger = logging.getLogger(\"wikitables\")\n",
    "logger.setLevel(logging.DEBUG)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# retrieve list of articles with tables in wikipedia\n",
    "# get 10 articles each from a-z with appeneded random letter to act as random sample of tables\n",
    "import string\n",
    "import random\n",
    "\n",
    "articles = list()\n",
    "site = mwclient.Site('en.wikipedia.org')\n",
    "\n",
    "# for letter in string.ascii_lowercase:\n",
    "#     counter = 0\n",
    "#     while (counter!=10):\n",
    "#         for page in site.allpages(filterredir='nonredirects', prefix=letter+random.choice(string.ascii_lowercase)):\n",
    "#             tables = mwp.parse(page.text()).filter_tags(matches=ftag('table'))\n",
    "#             if tables:\n",
    "#                 t = import_tables(page.name)\n",
    "#                 if t:\n",
    "#                     print(page.name)\n",
    "#                     articles.append(page.name)\n",
    "#                     counter+=1\n",
    "#                     break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# # save the articles with tables\n",
    "# import pickle\n",
    "\n",
    "# with open(\"articles.txt\", \"wb\") as fp:\n",
    "#     pickle.dump(articles, fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# import the saved articles\n",
    "import pickle\n",
    "with open(\"articles.txt\", \"rb\") as fp:\n",
    "    articles = sorted(list(set(pickle.load(fp))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ab-Soul discography\n",
      "AdJ\n",
      "Aedin Mincks\n",
      "Al-Ahli SC (Amman)\n",
      "An-Nabek District\n",
      "Ap'Tin Arhi: I Megaliteres Epitihies\n",
      "At-large\n",
      "Awa Pit language\n",
      "Az Khaneh Ta Goor\n",
      "Ba-Phalaborwa Local Municipality\n",
      "BcIII\n",
      "Bdellomicrovirus\n",
      "Bi-elliptic transfer\n",
      "Bj league\n",
      "Bnei Akiva\n",
      "Btissam Lakhouad\n",
      "BxB Hulk\n",
      "By-elections in Singapore\n",
      "Cc65\n",
      "CeCe Peniston discography\n",
      "Ch'ari\n",
      "CiCi Bellis\n",
      "Ckay1\n",
      "Cnidaria\n",
      "Co-Cathedral of Saint-Antoine-de-Padoue\n",
      "Ctenosaura bakeri\n",
      "Cy Touff\n",
      "Da'Vine Joy Randolph\n",
      "Db4o\n",
      "Dconf\n",
      "Dhaam Dhoom\n",
      "Do-Wacka-Do\n",
      "Dr. Ambedkar Government Law College, Chennai\n",
      "Dwadasama\n",
      "Eabametoong First Nation\n",
      "Ecatepec Region\n",
      "Eg (Kristiansand)\n",
      "Eh, Eh (Nothing Else I Can Say)\n",
      "Eja\n",
      "Ek: The Power of One\n",
      "EmArcy Records\n",
      "Es'kia Mphahlele\n",
      "Ex'Act\n",
      "Ey Reqîb\n",
      "Fa'a'ā International Airport\n",
      "FeONIC\n",
      "Ffestiniog Railway rolling stock\n",
      "Fgura United F.C.\n",
      "Flabellina bertschi\n",
      "Fm and TV Mast Chwaszczyno\n",
      "Fr. Conceicao Rodrigues Memorial Debate\n",
      "Ga-Segonyana Local Municipality\n",
      "Gdańsk Biskupia Górka railway station\n",
      "Gman//HJYUGTF2\n",
      "Graakalbanen\n",
      "Gu Changwei\n",
      "Gvat\n",
      "Ha'esh Hayaty\n",
      "He'ao station\n",
      "Hi! Pristin\n",
      "Hjalmar Andersen\n",
      "Ho-Hum\n",
      "Hsieh Pei-chen\n",
      "Ia Ora 'O Tahiti Nui\n",
      "IcCube\n",
      "If... (Tohoshinki song)\n",
      "In-A-Gadda-Da-Vida (album)\n",
      "Io (English band)\n",
      "Is'Thunzi\n",
      "It's (EP)\n",
      "Iwaizumi, Iwate\n",
      "Iya Villania\n",
      "Ja'Net DuBois\n",
      "Ji'an\n",
      "Jlime\n",
      "Jream Baby Jream\n",
      "JsonML\n",
      "Ju-jitsu at the 2009 Asian Indoor Games\n",
      "Jwaar Bhata\n",
      "Jyeshta (film)\n",
      "Kha (Indic)\n",
      "Kkochi\n",
      "Kl. 10\n",
      "Kmetija\n",
      "Knattspyrnudeild Keflavík\n",
      "Kpandai (Ghana parliament constituency)\n",
      "Ksar Hellal Congress\n",
      "Ku (kana)\n",
      "Kvadraturen (Kristiansand)\n",
      "Ky Laffoon\n",
      "Lhasa\n",
      "Li'l Abner (1959 film)\n",
      "Ljig\n",
      "Llama\n",
      "Lt. Ray Enners Award\n",
      "Lu (duo)\n",
      "Lviv\n",
      "LyX\n",
      "Mdina Knights F.C.\n",
      "Mhai\n",
      "Mi'gmawei Mawiomi Secretariat\n",
      "Mjölby AI FF\n",
      "MkLinux\n",
      "Mpade language\n",
      "Mt. Ararat High School\n",
      "Mwai Kibaki\n",
      "Na'vi River Journey\n",
      "Ne-Yo: The Collection\n",
      "Ng Chin Han\n",
      "Nha Sentimento\n",
      "Nkandla Local Municipality\n",
      "Ntabankulu Local Municipality\n",
      "NxWorries\n",
      "Nzadi language\n",
      "Ocacia\n",
      "Oh! (Girls' Generation song)\n",
      "Oj, svijetla majska zoro\n",
      "On-base plus slugging\n",
      "Oquendo-class destroyer\n",
      "OsCommerce\n",
      "Otabek Shukurov\n",
      "Ou Chum (commune)\n",
      "OzAsia Festival\n",
      "Pa'l Mundo\n",
      "Pbunalikevirus\n",
      "Pfeiffer Falcons\n",
      "Ph.D. (band)\n",
      "Pi Delta Psi\n",
      "PnB Rock discography\n",
      "PraSankar\n",
      "Pwllheli Lifeboat Station\n",
      "Pyaar Diwana Hota Hai\n",
      "Qaarsorsuaq Island\n",
      "QepHom\n",
      "Qi (state)\n",
      "Qntal\n",
      "QoS Class Identifier\n",
      "Qri\n",
      "Qzone\n",
      "Ra'ed Al-Nawateer\n",
      "Rh blood group system\n",
      "Ri-verbs\n",
      "Rjana Łužica\n",
      "RoPS\n",
      "Ry Bradley\n",
      "Sbarro restaurant suicide bombing\n",
      "Scaddan Ministry\n",
      "Sd.Kfz. 247\n",
      "Se Amar Mon Kereche\n",
      "SmackDown (WWE brand)\n",
      "Ssajib\n",
      "Su-Hyun Oh\n",
      "Svag doft av skymning\n",
      "Szabina Szlavikovics\n",
      "Tjaart van der Walt\n",
      "Tlajomulco de Zúñiga\n",
      "Ts'ili\n",
      "Tu (Umberto Tozzi song)\n",
      "Twan Castelijns\n",
      "Ty - supermodel\n",
      "Tzanata\n",
      "Ua discography\n",
      "Uda-class oiler\n",
      "Uecker-Randow\n",
      "Ujjayinee Roy\n",
      "Ukai Thermal Power Station\n",
      "Up! (Shania Twain song)\n",
      "Ur So Gay\n",
      "Us & Them\n",
      "Uta ga Chikara\n",
      "Va Sokthorn\n",
      "VfB Friedrichshafen\n",
      "Vhembe District Municipality\n",
      "Vo Centar\n",
      "Vrachnaiika\n",
      "Vu+\n",
      "VyOS\n",
      "Wo (kana)\n",
      "Wrangell Airport\n",
      "Wu-Chronicles\n",
      "Www.memory\n",
      "Xcelencia\n",
      "Xedio\n",
      "Xfce\n",
      "Xi (alternate reality game)\n",
      "Xocchel Municipality\n",
      "Xplore M98\n",
      "Xtampak\n",
      "Xx (album)\n",
      "Yi-Liu dialect\n",
      "Ylena In-Albon\n",
      "Yngling\n",
      "Yo! Bum Rush the Show\n",
      "Ypati\n",
      "Ysaline Bonaventure\n",
      "Yws Gwynedd\n",
      "Zhabdrung Rinpoche\n",
      "Zoa Morani\n",
      "Zrenjanin\n",
      "Zschaitz-Ottewig\n",
      "Zwartowo railway station\n",
      "Zydis\n"
     ]
    }
   ],
   "source": [
    "# extract tables and content needed for disambiguation/features\n",
    "t = nested_dict()\n",
    "\n",
    "# for i in range(len(articles)):\n",
    "#    t[i] = import_tables(articles[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# # save the extracted tables\n",
    "# import dill #lambda functions\n",
    "\n",
    "# with open(\"tables.txt\", \"wb\") as fp:\n",
    "#      dill.dump(t, fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# import the saved extracted tables\n",
    "import dill\n",
    "with open(\"tables.txt\", \"rb\") as fp:\n",
    "    extracted_tables = dill.load(fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "meal -> Meal (score: 0.2014230340719223)\n",
      "Mexican -> Mexican cuisine (score: 0.36614900827407837)\n",
      "burritos -> Burrito (score: 0.28607892990112305)\n"
     ]
    }
   ],
   "source": [
    "# disambiguate entities within cells\n",
    "import tagme\n",
    "\n",
    "with open(\"tagme\", 'r') as file:\n",
    "    token = file.readline().strip()\n",
    "    \n",
    "tagme.GCUBE_TOKEN = token\n",
    "\n",
    "lunch_annotations = tagme.annotate(\"My favourite meal is Mexican burritos.\")\n",
    "\n",
    "# Print annotations with a score higher than 0.1\n",
    "for ann in lunch_annotations.get_annotations(0.1):\n",
    "    print(ann)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'str' object has no attribute 'rows'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-8-1612955d79a6>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      6\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mv1\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0mdefaultdict\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m                 \u001b[0;32mfor\u001b[0m \u001b[0mk2\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mv2\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mv1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m                     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mv2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrows\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;31m# only extract triples that don't timeout\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'str' object has no attribute 'rows'"
     ]
    }
   ],
   "source": [
    "# extract RDF triples\n",
    "# use SPARQL queries\n",
    "for k,v in t.items():\n",
    "    if type(v) is defaultdict:\n",
    "        for k1,v1 in v.items():\n",
    "            if type(v1) is defaultdict:\n",
    "                for k2,v2 in v1.items():\n",
    "                    print(v2.rows)\n",
    "\n",
    "# only extract triples that don't timeout\n",
    "\n",
    "from SPARQLWrapper import SPARQLWrapper, JSON\n",
    "sparql = SPARQLWrapper(\"https://query.wikidata.org/sparql\")\n",
    "sparql.setQuery(\"\"\"SELECT * WHERE\n",
    "{\n",
    "     wd:Q2 ?p ?o .\n",
    "     FILTER(STRSTARTS(str(?p), \"http://www.wikidata.org/prop/direct/\"))\n",
    "     SERVICE wikibase:label { bd:serviceParam wikibase:language \"en\" }\n",
    "}\"\"\")\n",
    "sparql.setReturnFormat(JSON)\n",
    "results = sparql.query().convert()\n",
    "\n",
    "for result in results[\"results\"][\"bindings\"]:\n",
    "    print(result)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda env:anaconda3]",
   "language": "python",
   "name": "conda-env-anaconda3-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
