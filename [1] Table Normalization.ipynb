{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# import the 360 extracted tables from 198 articles\n",
    "import pickle\n",
    "import sys\n",
    "sys.setrecursionlimit(10000)\n",
    "with open(\"tables.txt\", \"rb\") as fp:\n",
    "    extracted_tables = pickle.load(fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import itertools\n",
    "\n",
    "def splitDict(d):\n",
    "    n = len(d) // 2          # length of smaller half\n",
    "    i = iter(d.items())      # alternatively, i = d.iteritems() works in Python 2\n",
    "\n",
    "    d1 = dict(itertools.islice(i, n))   # grab first n items\n",
    "    d2 = dict(i)                        # grab the rest\n",
    "\n",
    "    return d1, d2\n",
    "\n",
    "extracted_tables1, extracted_tables2 = splitDict(extracted_tables)\n",
    "extracted_tables1_1, extracted_tables1_2 = splitDict(extracted_tables1)\n",
    "extracted_tables2_1, extracted_tables2_2 = splitDict(extracted_tables2)\n",
    "\n",
    "# save the split tables to normalize by running 4 scripts concurrently\n",
    "import pickle\n",
    "\n",
    "# https://stackoverflow.com/a/26496899\n",
    "def default_to_regular(d):\n",
    "    if isinstance(d, defaultdict):\n",
    "        d = {k: default_to_regular(v) for k, v in d.items()}\n",
    "    return d\n",
    "\n",
    "extracted_tables1_1 = default_to_regular(extracted_tables1_1)\n",
    "extracted_tables1_2 = default_to_regular(extracted_tables1_2)\n",
    "extracted_tables2_1 = default_to_regular(extracted_tables2_1)\n",
    "extracted_tables2_2 = default_to_regular(extracted_tables2_2)\n",
    "\n",
    "with open(\"tables1_1.txt\", \"wb\") as fp:\n",
    "     pickle.dump(extracted_tables1_1, fp)\n",
    "\n",
    "with open(\"tables1_2.txt\", \"wb\") as fp:\n",
    "     pickle.dump(extracted_tables1_2, fp)\n",
    "        \n",
    "with open(\"tables2_1.txt\", \"wb\") as fp:\n",
    "     pickle.dump(extracted_tables2_1, fp)        \n",
    "        \n",
    "with open(\"tables2_2.txt\", \"wb\") as fp:\n",
    "     pickle.dump(extracted_tables2_2, fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# functions disambiguate entities using API call\n",
    "import tagme\n",
    "\n",
    "with open(\"tagme\", 'r') as file:\n",
    "    token = file.readline().strip()\n",
    "    \n",
    "tagme.GCUBE_TOKEN = token\n",
    "\n",
    "def disambig(text, min_rho=None):\n",
    "    annotations = tagme.annotate(text)\n",
    "    a = dict()\n",
    "    for x in annotations.annotations:\n",
    "        if min_rho is None or x.score > min_rho:\n",
    "            a[str(x.mention)] = x.entity_title\n",
    "        \n",
    "    return a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# check if normalzied table is empty\n",
    "# https://stackoverflow.com/a/1605679\n",
    "def isListEmpty(inList):\n",
    "    if isinstance(inList, list): # Is a list\n",
    "        return all( map(isListEmpty, inList) )\n",
    "    return False # Not a list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# This script was put into normalize_tables.py to run concurrently on 50 articles at a time to speed up the process\n",
    "# iterate through tables and normalize\n",
    "# filter for tables >= 2x2 (196 total)\n",
    "\n",
    "import re\n",
    "from collections import defaultdict\n",
    "\n",
    "nested_dict = lambda: defaultdict(nested_dict)\n",
    "normalized_tables = nested_dict()\n",
    "\n",
    "for k,v in extracted_tables.items():\n",
    "    if type(v) is dict:\n",
    "        # we've hit an article\n",
    "        title = v['title']\n",
    "        summary = v['extract']\n",
    "        print(title)\n",
    "        for k1,v1 in v.items():\n",
    "            if type(v1) is dict:\n",
    "                # hit a section containing table(s)                \n",
    "                heading = v1[\"head\"]\n",
    "                text = v1[\"text\"]\n",
    "                                                \n",
    "                # iterate through tables\n",
    "                for k2,v2 in v1['table'].items():\n",
    "                    # start getting our features here\n",
    "                    rows_count = v2[\"rows_count\"]\n",
    "                    cols_count = v2[\"cols_count\"]\n",
    "                    if rows_count + 1 < 2 or cols_count < 2:\n",
    "                        # table is too small (< 2x2) so we iterate to next table\n",
    "                        # we are left with 196 tables\n",
    "                        continue\n",
    "                    else:\n",
    "                        # get the rows  and recreate the table with only disambiguated entities\n",
    "                        old_table = [[0 for x in range(cols_count)] for y in range(rows_count+1)] # add 1 for the header row\n",
    "                        new_table = [[0 for x in range(cols_count)] for y in range(rows_count+1)] # add 1 for the header row\n",
    "\n",
    "                        # map the header for the rows\n",
    "                        hmap = dict()\n",
    "                        # disambiguate the header\n",
    "                        for ic, h in enumerate(v2[\"head\"]):\n",
    "                            entities = list()\n",
    "                            e = re.findall(r'\\[([^]]*)\\]', h)\n",
    "                            if e:\n",
    "                                for l in e:\n",
    "                                    if \"http://\" in l or \"https://\" in l:\n",
    "                                        pass\n",
    "                                    else:\n",
    "                                        entities.append(l)\n",
    "                            else:\n",
    "                                # no links so we disambiguate with combining article title, section text & heading\n",
    "                                d = disambig(title + heading + text + h, 0.1)\n",
    "                                for original,entitytitle in d.items():\n",
    "                                    if original in h:\n",
    "                                        entities.append(entitytitle)\n",
    "                            \n",
    "                            hmap[h] = ic\n",
    "                                              \n",
    "                            old_table[0][ic] = h\n",
    "                            new_table[0][ic] = list(set(entities))\n",
    "                            \n",
    "                        \n",
    "                        for ir, v3 in enumerate(v2[\"rows\"], 1):                            \n",
    "                            # iterate through cells\n",
    "                            for k4, v4 in v3.items():\n",
    "                                # inside a cell\n",
    "                                entities = list()\n",
    "                                if v4[\"link\"]:\n",
    "                                    # extract the links\n",
    "                                    e = re.findall(r'\\[([^]]*)\\]',v4[\"value\"])\n",
    "                                    for l in e:\n",
    "                                        if \"http://\" in l or \"https://\" in l:\n",
    "                                            pass\n",
    "                                        else:\n",
    "                                            entities.append(l)\n",
    "\n",
    "                                # otherwise diambiguate via combining article title, section text & heading, \n",
    "                                # column header\n",
    "                                # if no entities have been extracted yet\n",
    "                                if not entities:\n",
    "                                    d = disambig(title + heading + text + k4 + v4[\"value\"], 0.1)\n",
    "                                    for original,entitytitle in d.items():\n",
    "                                        if original in v4[\"value\"]:\n",
    "                                            entities.append(entitytitle)\n",
    "                                            \n",
    "                                cellindex = hmap[k4]\n",
    "\n",
    "                                old_table[ir][cellindex] = v4[\"value\"]\n",
    "                                new_table[ir][cellindex] = list(set(entities))\n",
    "\n",
    "                        if not isListEmpty(new_table):\n",
    "                            normalized_tables[title][k2][\"old_table\"] = old_table\n",
    "                            normalized_tables[title][k2][\"new_table\"] = new_table\n",
    "                            normalized_tables[title][k2][\"section_title\"] = heading\n",
    "                            print(old_table)\n",
    "                            print(new_table)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# save the normalized tables\n",
    "import pickle\n",
    "\n",
    "# https://stackoverflow.com/a/26496899\n",
    "def default_to_regular(d):\n",
    "    if isinstance(d, defaultdict):\n",
    "        d = {k: default_to_regular(v) for k, v in d.items()}\n",
    "    return d\n",
    "\n",
    "normalized_tables = default_to_regular(normalized_tables)\n",
    "\n",
    "with open(\"normalized_tables.txt\", \"wb\") as fp:\n",
    "     pickle.dump(normalized_tables, fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda env:anaconda3]",
   "language": "python",
   "name": "conda-env-anaconda3-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
