{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1300"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# import the ??? extracted tables from 335 articles\n",
    "import pickle\n",
    "import sys\n",
    "sys.setrecursionlimit(10000)\n",
    "with open(\"new_tables.txt\", \"rb\") as fp:\n",
    "    extracted_tables = pickle.load(fp)\n",
    "    \n",
    "len(extracted_tables)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import itertools\n",
    "\n",
    "from collections import defaultdict\n",
    "\n",
    "def splitDict(d):\n",
    "    n = len(d) // 2          # length of smaller half\n",
    "    i = iter(d.items())      # alternatively, i = d.iteritems() works in Python 2\n",
    "\n",
    "    d1 = dict(itertools.islice(i, n))   # grab first n items\n",
    "    d2 = dict(i)                        # grab the rest\n",
    "\n",
    "    return d1, d2\n",
    "\n",
    "extracted_tables1, extracted_tables2 = splitDict(extracted_tables)\n",
    "extracted_tables1_1, extracted_tables1_2 = splitDict(extracted_tables1)\n",
    "extracted_tables2_1, extracted_tables2_2 = splitDict(extracted_tables2)\n",
    "\n",
    "extracted_tables1_11, extracted_tables1_21 = splitDict(extracted_tables1_1)\n",
    "extracted_tables1_12, extracted_tables1_22 = splitDict(extracted_tables1_2)\n",
    "\n",
    "extracted_tables2_11, extracted_tables2_21 = splitDict(extracted_tables2_1)\n",
    "extracted_tables2_12, extracted_tables2_22 = splitDict(extracted_tables2_2)\n",
    "\n",
    "\n",
    "\n",
    "# save the split tables to normalize by running 4 scripts concurrently\n",
    "import pickle\n",
    "\n",
    "# https://stackoverflow.com/a/26496899\n",
    "def default_to_regular(d):\n",
    "    if isinstance(d, defaultdict):\n",
    "        d = {k: default_to_regular(v) for k, v in d.items()}\n",
    "    return d\n",
    "\n",
    "extracted_tables1_11 = default_to_regular(extracted_tables1_11)\n",
    "extracted_tables1_12 = default_to_regular(extracted_tables1_12)\n",
    "\n",
    "extracted_tables1_21 = default_to_regular(extracted_tables1_21)\n",
    "extracted_tables1_22 = default_to_regular(extracted_tables1_22)\n",
    "\n",
    "extracted_tables2_11 = default_to_regular(extracted_tables2_11)\n",
    "extracted_tables2_12 = default_to_regular(extracted_tables2_12)\n",
    "\n",
    "extracted_tables2_21 = default_to_regular(extracted_tables2_21)\n",
    "extracted_tables2_22 = default_to_regular(extracted_tables2_22)\n",
    "\n",
    "\n",
    "with open(\"tables1_11.txt\", \"wb\") as fp:\n",
    "     pickle.dump(extracted_tables1_11, fp)\n",
    "with open(\"tables1_12.txt\", \"wb\") as fp:\n",
    "     pickle.dump(extracted_tables1_12, fp)\n",
    "\n",
    "with open(\"tables1_21.txt\", \"wb\") as fp:\n",
    "     pickle.dump(extracted_tables1_21, fp)\n",
    "with open(\"tables1_22.txt\", \"wb\") as fp:\n",
    "     pickle.dump(extracted_tables1_22, fp)\n",
    "        \n",
    "with open(\"tables2_11.txt\", \"wb\") as fp:\n",
    "     pickle.dump(extracted_tables2_11, fp)       \n",
    "with open(\"tables2_12.txt\", \"wb\") as fp:\n",
    "     pickle.dump(extracted_tables2_12, fp) \n",
    "        \n",
    "with open(\"tables2_21.txt\", \"wb\") as fp:\n",
    "     pickle.dump(extracted_tables2_21, fp)\n",
    "with open(\"tables2_22.txt\", \"wb\") as fp:\n",
    "     pickle.dump(extracted_tables2_22, fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# functions disambiguate entities using API call\n",
    "import tagme\n",
    "\n",
    "with open(\"tagme\", 'r') as file:\n",
    "    token = file.readline().strip()\n",
    "    \n",
    "tagme.GCUBE_TOKEN = token\n",
    "\n",
    "def disambig(text, min_rho=None):\n",
    "    annotations = tagme.annotate(text)\n",
    "    a = dict()\n",
    "    for x in annotations.annotations:\n",
    "        if min_rho is None or x.score > min_rho:\n",
    "            a[str(x.mention)] = x.entity_title\n",
    "        \n",
    "    return a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# check if normalzied table is empty\n",
    "# https://stackoverflow.com/a/1605679\n",
    "def isListEmpty(inList):\n",
    "    if isinstance(inList, list): # Is a list\n",
    "        return all( map(isListEmpty, inList) )\n",
    "    return False # Not a list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# This script was put into normalize_tables.py to run concurrently on 50 articles at a time to speed up the process\n",
    "# iterate through tables and normalize\n",
    "# filter for tables >= 2x2 (196 total)\n",
    "\n",
    "import re\n",
    "\n",
    "nested_dict = lambda: defaultdict(nested_dict)\n",
    "normalized_tables = nested_dict()\n",
    "\n",
    "for k,v in extracted_tables.items():\n",
    "    if type(v) is dict:\n",
    "        # we've hit an article\n",
    "        title = v['title']\n",
    "        summary = v['extract']\n",
    "        print(title)\n",
    "        for k1,v1 in v.items():\n",
    "            if type(v1) is dict:\n",
    "                # hit a section containing table(s)                \n",
    "                heading = v1[\"head\"]\n",
    "                text = v1[\"text\"]\n",
    "                                                \n",
    "                # iterate through tables\n",
    "                for k2,v2 in v1['table'].items():\n",
    "                    # start getting our features here\n",
    "                    rows_count = v2[\"rows_count\"]\n",
    "                    cols_count = v2[\"cols_count\"]\n",
    "                    if rows_count + 1 < 2 or cols_count < 2:\n",
    "                        # table is too small (< 2x2) so we iterate to next table\n",
    "                        # we are left with 196 tables\n",
    "                        continue\n",
    "                    else:\n",
    "                        # get the rows  and recreate the table with only disambiguated entities\n",
    "                        old_table = [[0 for x in range(cols_count)] for y in range(rows_count+1)] # add 1 for the header row\n",
    "                        new_table = [[0 for x in range(cols_count)] for y in range(rows_count+1)] # add 1 for the header row\n",
    "\n",
    "                        # map the header for the rows\n",
    "                        hmap = dict()\n",
    "                        # disambiguate the header\n",
    "                        for ic, h in enumerate(v2[\"head\"]):\n",
    "                            entities = list()\n",
    "                            e = re.findall(r'\\[([^]]*)\\]', h)\n",
    "                            if e:\n",
    "                                for l in e:\n",
    "                                    if \"http://\" in l or \"https://\" in l:\n",
    "                                        pass\n",
    "                                    else:\n",
    "                                        entities.append(l)\n",
    "                            else:\n",
    "                                # no links so we disambiguate with combining article title, section text & heading\n",
    "                                d = disambig(title + heading + text + h, 0.1)\n",
    "                                for original,entitytitle in d.items():\n",
    "                                    if original in h:\n",
    "                                        entities.append(entitytitle)\n",
    "                            \n",
    "                            hmap[h] = ic\n",
    "                                              \n",
    "                            old_table[0][ic] = h\n",
    "                            new_table[0][ic] = list(set(entities))\n",
    "                            \n",
    "                        \n",
    "                        for ir, v3 in enumerate(v2[\"rows\"], 1):                            \n",
    "                            # iterate through cells\n",
    "                            for k4, v4 in v3.items():\n",
    "                                # inside a cell\n",
    "                                entities = list()\n",
    "                                if v4[\"link\"]:\n",
    "                                    # extract the links\n",
    "                                    e = re.findall(r'\\[([^]]*)\\]',v4[\"value\"])\n",
    "                                    for l in e:\n",
    "                                        if \"http://\" in l or \"https://\" in l:\n",
    "                                            pass\n",
    "                                        else:\n",
    "                                            entities.append(l)\n",
    "\n",
    "                                # otherwise diambiguate via combining article title, section text & heading, \n",
    "                                # column header\n",
    "                                # if no entities have been extracted yet\n",
    "                                if not entities:\n",
    "                                    d = disambig(title + heading + text + k4 + v4[\"value\"], 0.1)\n",
    "                                    for original,entitytitle in d.items():\n",
    "                                        if original in v4[\"value\"]:\n",
    "                                            entities.append(entitytitle)\n",
    "                                            \n",
    "                                cellindex = hmap[k4]\n",
    "\n",
    "                                old_table[ir][cellindex] = v4[\"value\"]\n",
    "                                new_table[ir][cellindex] = list(set(entities))\n",
    "\n",
    "                        if not isListEmpty(new_table):\n",
    "                            normalized_tables[title][k2][\"old_table\"] = old_table\n",
    "                            normalized_tables[title][k2][\"new_table\"] = new_table\n",
    "                            normalized_tables[title][k2][\"section_title\"] = heading\n",
    "                            print(old_table)\n",
    "                            print(new_table)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# save the normalized tables\n",
    "import pickle\n",
    "\n",
    "# https://stackoverflow.com/a/26496899\n",
    "def default_to_regular(d):\n",
    "    if isinstance(d, defaultdict):\n",
    "        d = {k: default_to_regular(v) for k, v in d.items()}\n",
    "    return d\n",
    "\n",
    "normalized_tables = default_to_regular(normalized_tables)\n",
    "\n",
    "with open(\"normalized_tables.txt\", \"wb\") as fp:\n",
    "     pickle.dump(normalized_tables, fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8\n"
     ]
    }
   ],
   "source": [
    "# Load the normalized tables\n",
    "\n",
    "import pickle\n",
    "import sys\n",
    "sys.setrecursionlimit(10000)\n",
    "\n",
    "nt = list()\n",
    "\n",
    "indi = [\"11\",\"12\",\"21\",\"22\"]\n",
    "\n",
    "for x in range(1,3):\n",
    "    for y in indi:\n",
    "        with open(\"normal_tables{}_{}.txt\".format(x,y), \"rb\") as fp:\n",
    "            nt.append(pickle.load(fp))\n",
    "            \n",
    "print(len(nt))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# # split tables again\n",
    "\n",
    "import itertools\n",
    "from collections import defaultdict\n",
    "\n",
    "def splitDict(d):\n",
    "    n = len(d) // 2          # length of smaller half\n",
    "    i = iter(d.items())      # alternatively, i = d.iteritems() works in Python 2\n",
    "\n",
    "    d1 = dict(itertools.islice(i, n))   # grab first n items\n",
    "    d2 = dict(i)                        # grab the rest\n",
    "\n",
    "    return d1, d2\n",
    "\n",
    "# save the split tables to normalize by running 4 scripts concurrently\n",
    "import pickle\n",
    "\n",
    "# https://stackoverflow.com/a/26496899\n",
    "def default_to_regular(d):\n",
    "    if isinstance(d, defaultdict):\n",
    "        d = {k: default_to_regular(v) for k, v in d.items()}\n",
    "    return d\n",
    "\n",
    "def splitDictWrite(d, name1, name2):\n",
    "    d1,d2 = splitDict(d)\n",
    "    d1 = default_to_regular(d1)\n",
    "    d2 = default_to_regular(d2)\n",
    "    \n",
    "    with open(name1, \"wb\") as fp:\n",
    "         pickle.dump(d1, fp)\n",
    "\n",
    "    with open(name2, \"wb\") as fp:\n",
    "         pickle.dump(d2, fp)\n",
    "            \n",
    "def loadDict(d):\n",
    "    with open(d, \"rb\") as fp:\n",
    "        n = pickle.load(fp)\n",
    "    return n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "2\n",
      "4\n",
      "6\n",
      "8\n",
      "10\n",
      "12\n",
      "14\n"
     ]
    }
   ],
   "source": [
    "splitDictWrite(nt[0],\"nt1_1\",\"nt1_2\")\n",
    "splitDictWrite(nt[1],\"nt2_1\",\"nt2_2\")\n",
    "splitDictWrite(nt[2],\"nt3_1\",\"nt3_2\")\n",
    "splitDictWrite(nt[3],\"nt4_1\",\"nt4_2\")\n",
    "splitDictWrite(nt[4],\"nt4_1\",\"nt4_2\")\n",
    "splitDictWrite(nt[5],\"nt4_1\",\"nt4_2\")\n",
    "splitDictWrite(nt[6],\"nt4_1\",\"nt4_2\")\n",
    "splitDictWrite(nt[7],\"nt4_1\",\"nt4_2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda env:anaconda3]",
   "language": "python",
   "name": "conda-env-anaconda3-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
