{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Load the normalized tables\n",
    "\n",
    "import pickle\n",
    "import sys\n",
    "sys.setrecursionlimit(10000)\n",
    "\n",
    "def loadDict(d):\n",
    "    with open(d, \"rb\") as fp:\n",
    "        n = pickle.load(fp)\n",
    "    return n\n",
    "\n",
    "nt1_1 = loadDict(\"nt1_1\")\n",
    "nt1_2 = loadDict(\"nt1_2\")\n",
    "nt2_1 = loadDict(\"nt2_1\")\n",
    "nt2_2 = loadDict(\"nt2_2\")\n",
    "nt3_1 = loadDict(\"nt3_1\")\n",
    "nt3_2 = loadDict(\"nt3_2\")\n",
    "nt4_1 = loadDict(\"nt4_1\")\n",
    "nt4_2 = loadDict(\"nt4_2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# set up SPARQL endpoint for wikidata\n",
    "from SPARQLWrapper import SPARQLWrapper, JSON\n",
    "sparql = SPARQLWrapper(\"https://query.wikidata.org/sparql\")\n",
    "\n",
    "def getPredicates(subject,obj, number = False):\n",
    "    if number:\n",
    "        # we do a different query and return only the non-inverse\n",
    "        sparql.setQuery(\"\"\"SELECT * WHERE\n",
    "        {\n",
    "             %s ?p %s .\n",
    "             FILTER(STRSTARTS(str(?p), \"http://www.wikidata.org/prop/direct/\"))\n",
    "             SERVICE wikibase:label { bd:serviceParam wikibase:language \"en\" }\n",
    "        }\"\"\" % (subject, obj))\n",
    "        \n",
    "        sparql.setReturnFormat(JSON)\n",
    "        results = sparql.query().convert()\n",
    "        \n",
    "        predicates = list()\n",
    "\n",
    "        for row in results[\"results\"][\"bindings\"]:\n",
    "            if row[\"p\"][\"type\"] == \"uri\":\n",
    "                predicates.append(row[\"p\"][\"value\"])\n",
    "\n",
    "        return predicates\n",
    "    \n",
    "    sparql.setQuery(\"\"\"SELECT DISTINCT ?p1 ?p2\n",
    "    {\n",
    "         {%s ?p1 %s \n",
    "         FILTER(STRSTARTS(str(?p1), \"http://www.wikidata.org/prop/direct/\"))} \n",
    "         UNION {%s ?p2 %s\n",
    "         FILTER(STRSTARTS(str(?p2), \"http://www.wikidata.org/prop/direct/\"))}\n",
    "         SERVICE wikibase:label { bd:serviceParam wikibase:language \"en\" }\n",
    "    }\"\"\" % (subject, obj, obj, subject))\n",
    "    \n",
    "    sparql.setReturnFormat(JSON)\n",
    "    results = sparql.query().convert()\n",
    "    \n",
    "    predicates_so = list()\n",
    "    predicates_os = list()\n",
    "    \n",
    "    for row in results[\"results\"][\"bindings\"]:\n",
    "        try:\n",
    "            if row[\"p1\"][\"type\"] == \"uri\":\n",
    "                predicates_so.append(row[\"p1\"][\"value\"])\n",
    "            if row[\"p2\"][\"type\"] == \"uri\":\n",
    "                predicates_os.append(row[\"p2\"][\"value\"])\n",
    "        except:\n",
    "            pass\n",
    "\n",
    "    return predicates_so, predicates_os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# resolve Wikidata entity from title\n",
    "\n",
    "from wikitables.client import Client\n",
    "\n",
    "client = Client(\"en\")\n",
    "\n",
    "def getWikidata(title):\n",
    "    return client.fetch_wikidata(title)\n",
    "\n",
    "def retrieveExtract(article):\n",
    "    return client.fetch_extract(article)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import simplemediawiki \n",
    "\n",
    "wiki = simplemediawiki.MediaWiki('https://www.wikidata.org/w/api.php')\n",
    "\n",
    "def findWDTitle(string, amount = None):\n",
    "    results = wiki.call({'action': 'wbsearchentities', 'search': string, 'type': 'property', 'language': 'en', 'limit': 10})\n",
    "\n",
    "    return results['search'][0]['label']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# functions entitiy relatedness using API call\n",
    "import tagme\n",
    "\n",
    "with open(\"tagme\", 'r') as file:\n",
    "    token = file.readline().strip()\n",
    "    \n",
    "tagme.GCUBE_TOKEN = token\n",
    "\n",
    "def disambig(text, min_rho=None):\n",
    "    annotations = tagme.annotate(text)\n",
    "    a = dict()\n",
    "    for x in annotations.annotations:\n",
    "        if min_rho is None or x.score > min_rho:\n",
    "            a[str(x.mention)] = x.entity_title\n",
    "        \n",
    "    return a\n",
    "\n",
    "# Get relatedness between a pair of entities specified by title.\n",
    "# rels = tagme.relatedness_title((\"Barack Obama\", \"Italy\"))\n",
    "# print(\"Obama and italy have a semantic relation of\", rels.relatedness[0].rel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Features (as suggested by the authors (Emir Munoz & Aidan Hogan) of Wikitables Triples Paper)\n",
    "# =============\n",
    "# Table Features\n",
    "#     (-) 1 Number of rows\n",
    "#     (-) 2 Number of columns\n",
    "#     (-) 3 Total relations extracted (all possible)\n",
    "# Column Features\n",
    "#     (+) 4 Potential relations (relations that do not exist)\n",
    "#     (+) 5 Unique potential relations (unique relations that do not exist)\n",
    "#     (+) 6 Entity relatedness (new)\n",
    "# Predicate Features\n",
    "#     (+) 7 Normalized unique subject count / Normalized unique object count\n",
    "#           normalized = unique X out of all X\n",
    "# Cell Features\n",
    "#     (-) 8 Number of entities in subject cell\n",
    "#     (-) 9 Number of entities in object cell\n",
    "#     (-) 10 String length in subject cell\n",
    "#     (-) 11 String length in object cell\n",
    "# Predicate/Column Features\n",
    "#     (+) 12 Maximum between Jaro-Walker distance and dice coefficient\n",
    "#            how related is the predicate to the headers\n",
    "#     (+) 13 Number of rows where the relation holds\n",
    "#     (+) 14 Number of relations in KB for all possible relations\n",
    "#     (+) 15 Number of relations in KB for all unique relations\n",
    "# Where (+) signifies a positive feature & (-) signifies a negative feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import re\n",
    "def find_number(string):\n",
    "    return re.findall('\\d+', string)\n",
    "\n",
    "import itertools\n",
    "from collections import defaultdict\n",
    "nested_dict = lambda: defaultdict(nested_dict)\n",
    "\n",
    "def flatten(l):\n",
    "    return list(itertools.chain.from_iterable(l))\n",
    "\n",
    "def getSOPred(string1, string2, fnt, section_entities, title):\n",
    "    e1 = string1.replace(\"'\",'\"')\n",
    "    e2 = string2.replace(\"'\",'\"')\n",
    "    n1 = False\n",
    "    n2  = False\n",
    "    predicates_so = set()\n",
    "    predicates_os = set()\n",
    "    if string1 not in fnt and string1 not in section_entities and string1 != title:\n",
    "        # not an actual entity; double quote\n",
    "        e1 = \"'\" + e1 + \"'\"\n",
    "        n1 = True\n",
    "    else:\n",
    "        # must catch exception because forgot to parse out deadlinks\n",
    "        try:\n",
    "            e1 = \"wd:\" + getWikidata(e1)\n",
    "        except:\n",
    "            e1 = \"'\" + e1 + \"'\"\n",
    "            n1 = True\n",
    "    if string2 not in fnt and string2 not in section_entities and string2 != title:\n",
    "        # not an actual entity; double quote\n",
    "        e2 = \"'\" + e2 + \"'\"\n",
    "        n2 = True\n",
    "    else:\n",
    "        # must catch exception because forgot to parse out deadlinks\n",
    "        try:\n",
    "            e2 = \"wd:\" + getWikidata(e2)\n",
    "        except:\n",
    "            e2 = \"'\" + e2 + \"'\"\n",
    "            n2 = True\n",
    "\n",
    "    if n1 and n2:\n",
    "        return set(), set()\n",
    "\n",
    "    if not n1 and not n2:\n",
    "        pred1, pred2 = getPredicates(e1,e2)\n",
    "\n",
    "        if pred1:\n",
    "            # e1 ?p e2\n",
    "            for p1 in pred1:\n",
    "                predicates_so.add(p1)\n",
    "        if pred2:\n",
    "            # e2 ?p e1\n",
    "            for p1 in pred2:\n",
    "                predicates_os.add(p1)\n",
    "\n",
    "    elif n1 and not n2:\n",
    "        pred = getPredicates(e2,e1,True)\n",
    "        if pred:\n",
    "            for p1 in pred:\n",
    "                predicates_os.add(p1)\n",
    "\n",
    "    elif not n1 and n2:\n",
    "        pred = getPredicates(e1,e2,True)\n",
    "        if pred:\n",
    "            # e1 ?p e2\n",
    "            for p1 in pred:\n",
    "                predicates_so.add(p1)\n",
    "    \n",
    "    return predicates_so, predicates_os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Predicate/ Column Features\n",
    "# (+) 12 Max of dice coeffient and jaro-winkler distance\n",
    "from pyjarowinkler import distance\n",
    "\n",
    "def dice_coefficient(a,b):\n",
    "    if not len(a) or not len(b): return 0.0\n",
    "    \"\"\" quick case for true duplicates \"\"\"\n",
    "    if a == b: return 1.0\n",
    "    \"\"\" if a != b, and a or b are single chars, then they can't possibly match \"\"\"\n",
    "    if len(a) == 1 or len(b) == 1: return 0.0\n",
    "    \n",
    "    \"\"\" use python list comprehension, preferred over list.append() \"\"\"\n",
    "    a_bigram_list = [a[i:i+2] for i in range(len(a)-1)]\n",
    "    b_bigram_list = [b[i:i+2] for i in range(len(b)-1)]\n",
    "    \n",
    "    a_bigram_list.sort()\n",
    "    b_bigram_list.sort()\n",
    "    \n",
    "    # assignments to save function calls\n",
    "    lena = len(a_bigram_list)\n",
    "    lenb = len(b_bigram_list)\n",
    "    # initialize match counters\n",
    "    matches = i = j = 0\n",
    "    while (i < lena and j < lenb):\n",
    "        if a_bigram_list[i] == b_bigram_list[j]:\n",
    "            matches += 2\n",
    "            i += 1\n",
    "            j += 1\n",
    "        elif a_bigram_list[i] < b_bigram_list[j]:\n",
    "            i += 1\n",
    "        else:\n",
    "            j += 1\n",
    "    \n",
    "    score = float(matches)/float(lena + lenb)\n",
    "    return score\n",
    "\n",
    "def getfeature12(string1, string2):\n",
    "    return max(distance.get_jaro_distance(string1, string2, winkler=True, scaling=0.1),\\\n",
    "              dice_coefficient(string1, string2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "If... (Tohoshinki song)\n",
      "defaultdict(<class 'list'>, {('Oricon', 'Japan'): ['http://www.wikidata.org/prop/direct/P17'], ('Japan', 'Japan'): ['http://www.wikidata.org/prop/direct/P17']})\n"
     ]
    }
   ],
   "source": [
    "k = \"If... (Tohoshinki song)\"\n",
    "\n",
    "tables = nt1_2[k]\n",
    "\n",
    "# tables = nt1_2\n",
    "\n",
    "\n",
    "import csv\n",
    "\n",
    "file = \"test.txt\"\n",
    "#filegold = \"testgold.txt\"\n",
    "\n",
    "# for k,v in tables.items():\n",
    "#     print(k)\n",
    "#     for k1,v1 in v.items():\n",
    "for v in range(1):\n",
    "    print(k)\n",
    "    for k1,v1 in tables.items():\n",
    "        # hit table\n",
    "        old_table = v1[\"old_table\"]\n",
    "        new_table = v1[\"new_table\"]\n",
    "        section_title = v1[\"section_title\"]\n",
    "\n",
    "        len_rows = len(old_table)\n",
    "        len_cols = len(old_table[0])\n",
    "\n",
    "        temp_table = [[0 for y in range(len_cols)] for x in range(len_rows)]\n",
    "\n",
    "        # populate temp table (copy of new table) with values from original table if it hasn't been disambiguated\n",
    "        # also generate all possible positions\n",
    "        positions = nested_dict()\n",
    "        for x in range(len_rows):\n",
    "            for y in range(len_cols):\n",
    "                if not new_table[x][y]:\n",
    "                    # try to make the obj a number\n",
    "                    try:\n",
    "                        n = find_number(old_table[x][y])\n",
    "                    except:\n",
    "                        n = []\n",
    "                    if n:\n",
    "                        temp_table[x][y] = [str(n[0])]\n",
    "                    else:\n",
    "                        temp_table[x][y] = [str(old_table[x][y]).replace(\"'\",'\"')]\n",
    "                else:\n",
    "                    temp_table[x][y] = new_table[x][y]\n",
    "                    \n",
    "                len_cell = len(temp_table[x][y]) # how many elements are inside cell                \n",
    "                \n",
    "                cell_indices = list(itertools.product([y],list(range(len_cell))))\n",
    "  \n",
    "                if x > 0: # skip the header\n",
    "                    for ci in cell_indices:\n",
    "                        positions[x][ci] = temp_table[x][ci[0]][ci[1]]     \n",
    "                        \n",
    "        section_entities = list()\n",
    "        d = disambig(k + retrieveExtract(k) + section_title, 0.1)\n",
    "        for original, entitytitle in d.items():\n",
    "            if original in section_title:\n",
    "                section_entities.append(entitytitle)\n",
    "                \n",
    "        if section_entities:\n",
    "            # make up pos for section\n",
    "            for ise, se in enumerate(section_entities):\n",
    "                positions[\"section\"][(ise, \"section\")] = se\n",
    "        \n",
    "        # created new temp_table\n",
    "        # generate all relations based on pos\n",
    "        relations = list()\n",
    "        halfrelations = list() # only generate half to reduce SPARQL queries\n",
    "        fullpositions = list() # we keep a list of the positions inside the table\n",
    "                               # to create the cartesian producst later\n",
    "        for row, pos in positions.items():\n",
    "            if row != \"section\" or row != \"article\":\n",
    "                fullpos = list(itertools.product([row], pos.keys()))\n",
    "                fullpositions+=fullpos\n",
    "                relations+=list(itertools.permutations(fullpos,2))\n",
    "                halfrelations+=list(itertools.combinations(fullpos,2))\n",
    "                \n",
    "        # generate relations between section title and entities inside table\n",
    "        fullpos = list(itertools.product([\"section\"], positions[\"section\"].keys()))\n",
    "        relations+=list(itertools.product(fullpos, fullpositions))\n",
    "        halfrelations+=list(itertools.product(fullpos, fullpositions))\n",
    "        relations+=list(itertools.product(fullpositions, fullpos))\n",
    "        \n",
    "        # generate relations between article title and entities inside table\n",
    "        fullpos = [(\"article\", (k, \"article\"))]\n",
    "        relations+=list(itertools.product(fullpos, fullpositions))\n",
    "        halfrelations+=list(itertools.product(fullpos, fullpositions))\n",
    "        relations+=list(itertools.product(fullpositions, fullpos))      \n",
    "        \n",
    "            \n",
    "        resolved_relations = defaultdict(list)\n",
    "        resolved_halfrelations = defaultdict(list)\n",
    "        for r in relations:\n",
    "            if r[0][0] == \"article\":\n",
    "                subject = r[0][1][0]\n",
    "            elif r[0][0] == \"section\":\n",
    "                subject = section_entities[r[0][1][0]]\n",
    "            else:\n",
    "                subject = temp_table[r[0][0]][r[0][1][0]][r[0][1][1]]\n",
    "                \n",
    "            if r[1][0] == \"article\":\n",
    "                obj = r[1][1][0]\n",
    "            elif r[1][0] == \"section\":\n",
    "                obj = section_entities[r[1][1][0]]\n",
    "            else:\n",
    "                obj = temp_table[r[1][0]][r[1][1][0]][r[1][1][1]]\n",
    "                \n",
    "            resolved_relations[(subject,obj)].append(r)\n",
    "            \n",
    "        for r in halfrelations:\n",
    "            if r[0][0] == \"article\":\n",
    "                subject = r[0][1][0]\n",
    "            elif r[0][0] == \"section\":\n",
    "                subject = section_entities[r[0][1][0]]\n",
    "            else:\n",
    "                subject = temp_table[r[0][0]][r[0][1][0]][r[0][1][1]]\n",
    "                \n",
    "            if r[1][0] == \"article\":\n",
    "                obj = r[1][1][0]\n",
    "            elif r[1][0] == \"section\":\n",
    "                obj = section_entities[r[1][1][0]]\n",
    "            else:\n",
    "                obj = temp_table[r[1][0]][r[1][1][0]][r[1][1][1]]\n",
    "                \n",
    "            resolved_halfrelations[(subject,obj)].append(r)\n",
    "\n",
    "        # use half relations to find predicates\n",
    "        predicates = defaultdict(list)\n",
    "        for hr in resolved_halfrelations.keys():\n",
    "            # get all the rows to flatten\n",
    "            rows = list()\n",
    "            fnt = list()\n",
    "            for pos in resolved_halfrelations[hr]:\n",
    "                if isinstance(pos[0][0],int):\n",
    "                    rows.append(pos[0][0])\n",
    "                if isinstance(pos[1][0],int):\n",
    "                    rows.append(pos[1][0])\n",
    "            for r in rows:\n",
    "                fnt+=flatten([new_table[r]])\n",
    "                \n",
    "            try:                    \n",
    "                soPred, osPred = getSOPred(hr[0], hr[1], fnt, section_entities, k)\n",
    "            except:\n",
    "                continue\n",
    "                        \n",
    "            if soPred:\n",
    "                predicates[(hr[0],hr[1])]+=soPred\n",
    "            if osPred:\n",
    "                predicates[(hr[1],hr[0])]+=osPred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "defaultdict(<class 'list'>, {('Oricon', 'Japan'): ['http://www.wikidata.org/prop/direct/P17'], ('Japan', 'Japan'): ['http://www.wikidata.org/prop/direct/P17']})\n",
      "('Japan', 'Japan') ['http://www.wikidata.org/prop/direct/P17']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/k/anaconda3/lib/python3.5/site-packages/tagme/__init__.py:176: DeprecationWarning: The unescape method is deprecated and will be removed in 3.5, use html.unescape() instead.\n",
      "  return HTML_PARSER.unescape(title.strip(\" _\").replace(\"_\", \" \"))\n",
      "/home/k/anaconda3/lib/python3.5/site-packages/tagme/__init__.py:176: DeprecationWarning: The unescape method is deprecated and will be removed in 3.5, use html.unescape() instead.\n",
      "  return HTML_PARSER.unescape(title.strip(\" _\").replace(\"_\", \" \"))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Japan country Japan\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/k/anaconda3/lib/python3.5/site-packages/tagme/__init__.py:176: DeprecationWarning: The unescape method is deprecated and will be removed in 3.5, use html.unescape() instead.\n",
      "  return HTML_PARSER.unescape(title.strip(\" _\").replace(\"_\", \" \"))\n",
      "/home/k/anaconda3/lib/python3.5/site-packages/tagme/__init__.py:176: DeprecationWarning: The unescape method is deprecated and will be removed in 3.5, use html.unescape() instead.\n",
      "  return HTML_PARSER.unescape(title.strip(\" _\").replace(\"_\", \" \"))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Japan country Japan\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/k/anaconda3/lib/python3.5/site-packages/tagme/__init__.py:176: DeprecationWarning: The unescape method is deprecated and will be removed in 3.5, use html.unescape() instead.\n",
      "  return HTML_PARSER.unescape(title.strip(\" _\").replace(\"_\", \" \"))\n",
      "/home/k/anaconda3/lib/python3.5/site-packages/tagme/__init__.py:176: DeprecationWarning: The unescape method is deprecated and will be removed in 3.5, use html.unescape() instead.\n",
      "  return HTML_PARSER.unescape(title.strip(\" _\").replace(\"_\", \" \"))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5 country Japan\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/k/anaconda3/lib/python3.5/site-packages/tagme/__init__.py:176: DeprecationWarning: The unescape method is deprecated and will be removed in 3.5, use html.unescape() instead.\n",
      "  return HTML_PARSER.unescape(title.strip(\" _\").replace(\"_\", \" \"))\n",
      "/home/k/anaconda3/lib/python3.5/site-packages/tagme/__init__.py:176: DeprecationWarning: The unescape method is deprecated and will be removed in 3.5, use html.unescape() instead.\n",
      "  return HTML_PARSER.unescape(title.strip(\" _\").replace(\"_\", \" \"))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5 country Japan\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/k/anaconda3/lib/python3.5/site-packages/tagme/__init__.py:176: DeprecationWarning: The unescape method is deprecated and will be removed in 3.5, use html.unescape() instead.\n",
      "  return HTML_PARSER.unescape(title.strip(\" _\").replace(\"_\", \" \"))\n",
      "/home/k/anaconda3/lib/python3.5/site-packages/tagme/__init__.py:176: DeprecationWarning: The unescape method is deprecated and will be removed in 3.5, use html.unescape() instead.\n",
      "  return HTML_PARSER.unescape(title.strip(\" _\").replace(\"_\", \" \"))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "18689 country Japan\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/k/anaconda3/lib/python3.5/site-packages/tagme/__init__.py:176: DeprecationWarning: The unescape method is deprecated and will be removed in 3.5, use html.unescape() instead.\n",
      "  return HTML_PARSER.unescape(title.strip(\" _\").replace(\"_\", \" \"))\n",
      "/home/k/anaconda3/lib/python3.5/site-packages/tagme/__init__.py:176: DeprecationWarning: The unescape method is deprecated and will be removed in 3.5, use html.unescape() instead.\n",
      "  return HTML_PARSER.unescape(title.strip(\" _\").replace(\"_\", \" \"))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12 country Japan\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/k/anaconda3/lib/python3.5/site-packages/tagme/__init__.py:176: DeprecationWarning: The unescape method is deprecated and will be removed in 3.5, use html.unescape() instead.\n",
      "  return HTML_PARSER.unescape(title.strip(\" _\").replace(\"_\", \" \"))\n",
      "/home/k/anaconda3/lib/python3.5/site-packages/tagme/__init__.py:176: DeprecationWarning: The unescape method is deprecated and will be removed in 3.5, use html.unescape() instead.\n",
      "  return HTML_PARSER.unescape(title.strip(\" _\").replace(\"_\", \" \"))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5 country Japan\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/k/anaconda3/lib/python3.5/site-packages/tagme/__init__.py:176: DeprecationWarning: The unescape method is deprecated and will be removed in 3.5, use html.unescape() instead.\n",
      "  return HTML_PARSER.unescape(title.strip(\" _\").replace(\"_\", \" \"))\n",
      "/home/k/anaconda3/lib/python3.5/site-packages/tagme/__init__.py:176: DeprecationWarning: The unescape method is deprecated and will be removed in 3.5, use html.unescape() instead.\n",
      "  return HTML_PARSER.unescape(title.strip(\" _\").replace(\"_\", \" \"))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "30 country Japan\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/k/anaconda3/lib/python3.5/site-packages/tagme/__init__.py:176: DeprecationWarning: The unescape method is deprecated and will be removed in 3.5, use html.unescape() instead.\n",
      "  return HTML_PARSER.unescape(title.strip(\" _\").replace(\"_\", \" \"))\n",
      "/home/k/anaconda3/lib/python3.5/site-packages/tagme/__init__.py:176: DeprecationWarning: The unescape method is deprecated and will be removed in 3.5, use html.unescape() instead.\n",
      "  return HTML_PARSER.unescape(title.strip(\" _\").replace(\"_\", \" \"))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "18689 country Japan\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/k/anaconda3/lib/python3.5/site-packages/tagme/__init__.py:176: DeprecationWarning: The unescape method is deprecated and will be removed in 3.5, use html.unescape() instead.\n",
      "  return HTML_PARSER.unescape(title.strip(\" _\").replace(\"_\", \" \"))\n",
      "/home/k/anaconda3/lib/python3.5/site-packages/tagme/__init__.py:176: DeprecationWarning: The unescape method is deprecated and will be removed in 3.5, use html.unescape() instead.\n",
      "  return HTML_PARSER.unescape(title.strip(\" _\").replace(\"_\", \" \"))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12 country Japan\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/k/anaconda3/lib/python3.5/site-packages/tagme/__init__.py:176: DeprecationWarning: The unescape method is deprecated and will be removed in 3.5, use html.unescape() instead.\n",
      "  return HTML_PARSER.unescape(title.strip(\" _\").replace(\"_\", \" \"))\n",
      "/home/k/anaconda3/lib/python3.5/site-packages/tagme/__init__.py:176: DeprecationWarning: The unescape method is deprecated and will be removed in 3.5, use html.unescape() instead.\n",
      "  return HTML_PARSER.unescape(title.strip(\" _\").replace(\"_\", \" \"))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5 country Japan\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/k/anaconda3/lib/python3.5/site-packages/tagme/__init__.py:176: DeprecationWarning: The unescape method is deprecated and will be removed in 3.5, use html.unescape() instead.\n",
      "  return HTML_PARSER.unescape(title.strip(\" _\").replace(\"_\", \" \"))\n",
      "/home/k/anaconda3/lib/python3.5/site-packages/tagme/__init__.py:176: DeprecationWarning: The unescape method is deprecated and will be removed in 3.5, use html.unescape() instead.\n",
      "  return HTML_PARSER.unescape(title.strip(\" _\").replace(\"_\", \" \"))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25702 country Japan\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/k/anaconda3/lib/python3.5/site-packages/tagme/__init__.py:176: DeprecationWarning: The unescape method is deprecated and will be removed in 3.5, use html.unescape() instead.\n",
      "  return HTML_PARSER.unescape(title.strip(\" _\").replace(\"_\", \" \"))\n",
      "/home/k/anaconda3/lib/python3.5/site-packages/tagme/__init__.py:176: DeprecationWarning: The unescape method is deprecated and will be removed in 3.5, use html.unescape() instead.\n",
      "  return HTML_PARSER.unescape(title.strip(\" _\").replace(\"_\", \" \"))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "286 country Japan\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/k/anaconda3/lib/python3.5/site-packages/tagme/__init__.py:176: DeprecationWarning: The unescape method is deprecated and will be removed in 3.5, use html.unescape() instead.\n",
      "  return HTML_PARSER.unescape(title.strip(\" _\").replace(\"_\", \" \"))\n",
      "/home/k/anaconda3/lib/python3.5/site-packages/tagme/__init__.py:176: DeprecationWarning: The unescape method is deprecated and will be removed in 3.5, use html.unescape() instead.\n",
      "  return HTML_PARSER.unescape(title.strip(\" _\").replace(\"_\", \" \"))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "30 country Japan\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/k/anaconda3/lib/python3.5/site-packages/tagme/__init__.py:176: DeprecationWarning: The unescape method is deprecated and will be removed in 3.5, use html.unescape() instead.\n",
      "  return HTML_PARSER.unescape(title.strip(\" _\").replace(\"_\", \" \"))\n",
      "/home/k/anaconda3/lib/python3.5/site-packages/tagme/__init__.py:176: DeprecationWarning: The unescape method is deprecated and will be removed in 3.5, use html.unescape() instead.\n",
      "  return HTML_PARSER.unescape(title.strip(\" _\").replace(\"_\", \" \"))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "18689 country Japan\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/k/anaconda3/lib/python3.5/site-packages/tagme/__init__.py:176: DeprecationWarning: The unescape method is deprecated and will be removed in 3.5, use html.unescape() instead.\n",
      "  return HTML_PARSER.unescape(title.strip(\" _\").replace(\"_\", \" \"))\n",
      "/home/k/anaconda3/lib/python3.5/site-packages/tagme/__init__.py:176: DeprecationWarning: The unescape method is deprecated and will be removed in 3.5, use html.unescape() instead.\n",
      "  return HTML_PARSER.unescape(title.strip(\" _\").replace(\"_\", \" \"))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12 country Japan\n"
     ]
    }
   ],
   "source": [
    "        if predicates:\n",
    "            print(predicates)\n",
    "            # extract features and write to csv\n",
    "            feature1 = len(temp_table) - 1 # do not count header\n",
    "            feature2 = len(temp_table[0])\n",
    "            feature3 = len(relations)\n",
    "\n",
    "            lp = list()\n",
    "            for x in predicates.keys():\n",
    "                lp+=resolved_relations[x]\n",
    "            feature4 = len(relations) - len(lp)\n",
    "\n",
    "            uniquepotential = set(relations) - set(lp)\n",
    "            feature5 = len(uniquepotential)\n",
    "\n",
    "            # extract feature 6 when writing specific relations\n",
    "\n",
    "            allsubjects = defaultdict(int)\n",
    "            allobjects = defaultdict(int)\n",
    "            for r in resolved_relations.keys():\n",
    "                factor = len(resolved_relations[r])\n",
    "                allsubjects[r[0]]+=factor\n",
    "                allobjects[r[1]]+=factor\n",
    "\n",
    "#             feature7 = (len(allsubjects.keys()) / sum(allsubjects.values())) / (len(allobjects.keys()) / sum(allobjects.values()))\n",
    "            feature7 = (len(allsubjects.keys()) / sum(allsubjects.values())) # it will be the same for objects since we try all combinations\n",
    "\n",
    "             # write the gold standard triples\n",
    "\n",
    "            to_write = set()\n",
    "\n",
    "\n",
    "#                     print(subject,wdtitle,obj)\n",
    "#                     print(ip)\n",
    "\n",
    "            # features 6,8,9,10,11,12,13\n",
    "            for pk, pv in predicates.items(): \n",
    "                print(pk,pv)\n",
    "                subject = pk[0]\n",
    "                obj = pk[1]\n",
    "                subjectFlag = False\n",
    "                objFlag = False\n",
    "                for ip in pv:\n",
    "                    for rr in resolved_relations[pk]:\n",
    "                        if rr[0][0] == \"article\":\n",
    "                            subjectFlag = True\n",
    "                        elif rr[0][0] == \"section\":\n",
    "                            subjectFlag = True\n",
    "\n",
    "                        if rr[1][0] == \"article\":\n",
    "                            objFlag = True\n",
    "                        elif rr[1][0] == \"section\":\n",
    "                            objFlag = True\n",
    "\n",
    "                        try:\n",
    "                            relatedness = tagme.relatedness_title((subject, obj)).relatedness[0].rel\n",
    "                        except:\n",
    "                            relatedness = 0\n",
    "                        if relatedness:\n",
    "                            feature6 = float(relatedness)\n",
    "                        else:\n",
    "                            feature6 = 0\n",
    "\n",
    "                        # feature 8 and 10\n",
    "                        if subjectFlag:\n",
    "                            if rr[0][1] == \"article\":\n",
    "                                feature8 = 1\n",
    "                                feature10 = len(k)\n",
    "                            else:\n",
    "                                feature8 = len(section_entities)\n",
    "                                feature10 = len(section_title)\n",
    "                            subj_header = subject\n",
    "                        else:\n",
    "                            feature8 = len(temp_table[rr[0][0]][rr[0][1][0]])\n",
    "                            feature10 = len(old_table[rr[0][0]][rr[0][1][0]])\n",
    "\n",
    "                            subj_header = old_table[0][rr[0][1][0]]\n",
    "\n",
    "                        # feature 9 and 11\n",
    "                        if objFlag:\n",
    "                            if rr[1][1] == \"article\":\n",
    "                                feature9 = 1\n",
    "                                feature11 = len(k)\n",
    "                            else:\n",
    "                                feature9 = len(section_entities)\n",
    "                                feature11 = len(section_title)\n",
    "                            obj_header = obj\n",
    "\n",
    "                        else:\n",
    "                            feature9 = len(temp_table[rr[1][0]][rr[1][1][0]])\n",
    "                            feature11 = len(old_table[rr[1][0]][rr[1][1][0]])\n",
    "\n",
    "                            obj_header = old_table[0][rr[1][1][0]]\n",
    "\n",
    "\n",
    "                        # feature 12\n",
    "                        wdtitle = findWDTitle(ip)\n",
    "                        feature12 = max(getfeature12(subj_header,wdtitle),getfeature12(obj_header,wdtitle))\n",
    "\n",
    "                        # feature 13\n",
    "                        feature13 = len(resolved_relations[(subject,obj)])\n",
    "\n",
    "                        print(subject,wdtitle,obj)\n",
    "\n",
    "                        to_write.add((feature1,feature2,feature3,feature4,feature5,\\\n",
    "                                                    feature6,feature7,feature8,feature9,feature10,\\\n",
    "                                                    feature11,feature12,feature13,ip,subject,wdtitle,obj,1))            \n",
    "            \n",
    "            \n",
    "                potentialrows = set(range(1,len_rows))\n",
    "                subject_col = list()\n",
    "                object_col = list()\n",
    "\n",
    "                for ir in resolved_relations[pk]:\n",
    "                    potentialrows = potentialrows - set([ir[0][0]])\n",
    "                    if ir[0][0] != \"section\" and ir[0][0] != \"article\":\n",
    "                        subject_col.append(ir[0][1][0])\n",
    "                    if ir[1][0] != \"section\" and ir[1][0] != \"article\":\n",
    "                        object_col.append(ir[1][1][0])\n",
    "                    if ir[0][0] == \"section\" or ir[0][0] == \"article\":\n",
    "                        subject_col.append(ir[0][1])\n",
    "                    if ir[1][0] == \"section\" or ir[1][0] == \"article\":\n",
    "                        object_col.append(ir[1][1])\n",
    "                      \n",
    "                potential_subjects = set()\n",
    "                potential_objects = set()\n",
    "                \n",
    "                for row in potentialrows:\n",
    "                    # use uniquepotential in the future\n",
    "                    for sc in set(subject_col):\n",
    "                        potential_subjects = potential_subjects.union(set([(row, xsc)\\\n",
    "                                                                           for xsc in positions[row].keys()\\\n",
    "                                                                           if xsc[0]]))\n",
    "                        try:\n",
    "                            if sc[1] == \"article\" or sc[1] == \"section\":\n",
    "                                potential_objects.add(sc)    \n",
    "                        except:\n",
    "                            pass\n",
    "\n",
    "                    for oc in set(object_col):\n",
    "                        potential_objects = potential_objects.union(set([(row, xoc)\\\n",
    "                                                                         for xoc in positions[row].keys()\\\n",
    "                                                                         if xoc[0]])) \n",
    "                        try:                        \n",
    "                            if oc[1] == \"article\" or oc[1] == \"section\":\n",
    "                                potential_objects.add(oc)\n",
    "                        except:\n",
    "                            pass\n",
    "\n",
    "                    # generate cartesian product between potential subjects and potential columns\n",
    "                    combos = list(itertools.product(potential_subjects, potential_objects))\n",
    "                    \n",
    "                    # print(combos)\n",
    "                    for cc in combos:\n",
    "                        \n",
    "                        subjectFlag = False\n",
    "                        objFlag = False\n",
    "                        \n",
    "                        try:\n",
    "                            if not new_table[cc[0][0]][cc[0][1][0]] and not new_table[cc[1][0]][cc[1][1][0]]:\n",
    "                                continue\n",
    "                        except:\n",
    "                            # check to see if entity is from title or subsection\n",
    "                            if cc[0][1] == \"article\":\n",
    "                                subject = k\n",
    "                                subjectFlag = True\n",
    "                            elif cc[0][1] == \"section\":\n",
    "                                subject = section_entities[cc[0][0]]\n",
    "                                subjectFlag = True\n",
    "                            else:\n",
    "                                subject = temp_table[cc[0][0]][cc[0][1][0]][cc[0][1][1]]\n",
    "\n",
    "                            if cc[1][1] == \"article\":\n",
    "                                obj = k\n",
    "                                objFlag = True\n",
    "                            elif cc[1][1] == \"section\":\n",
    "                                obj = section_entities[cc[1][0]]\n",
    "                                objFlag = True\n",
    "                            else:                            \n",
    "                                obj = temp_table[cc[1][0]][cc[1][1][0]][cc[1][1][1]]\n",
    "\n",
    "                            if subject != obj and subject!='' and obj!='':\n",
    "                                try:\n",
    "                                    relatedness = tagme.relatedness_title((subject, obj)).relatedness[0].rel\n",
    "                                except:\n",
    "                                    relatedness = 0\n",
    "                                if relatedness:\n",
    "                                    feature6 = float(relatedness)\n",
    "                                else:\n",
    "                                    feature6 = 0\n",
    "\n",
    "                                # feature 8 and 10\n",
    "                                if subjectFlag:\n",
    "                                    if cc[0][1] == \"article\":\n",
    "                                        feature8 = 1\n",
    "                                        feature10 = len(k)\n",
    "                                    else:\n",
    "                                        feature8 = len(section_entities)\n",
    "                                        feature10 = len(section_title)\n",
    "                                    subj_header = subject\n",
    "                                else:\n",
    "                                    feature8 = len(temp_table[cc[0][0]][cc[0][1][0]])\n",
    "                                    feature10 = len(old_table[cc[0][0]][cc[0][1][0]])\n",
    "\n",
    "                                    subj_header = old_table[0][cc[0][1][0]]\n",
    "\n",
    "                                # feature 9 and 11\n",
    "                                if objFlag:\n",
    "                                    if cc[1][1] == \"article\":\n",
    "                                        feature9 = 1\n",
    "                                        feature11 = len(k)\n",
    "                                    else:\n",
    "                                        feature9 = len(section_entities)\n",
    "                                        feature11 = len(section_title)\n",
    "                                    obj_header = obj\n",
    "\n",
    "                                else:\n",
    "                                    feature9 = len(temp_table[cc[1][0]][cc[1][1][0]])\n",
    "                                    feature11 = len(old_table[cc[1][0]][cc[1][1][0]])\n",
    "\n",
    "                                    obj_header = old_table[0][cc[1][1][0]]\n",
    "\n",
    "\n",
    "                                # feature 12\n",
    "                                wdtitle = findWDTitle(pv[0].split('/')[-1])\n",
    "                                feature12 = max(getfeature12(subj_header,wdtitle),getfeature12(obj_header,wdtitle))\n",
    "\n",
    "                                # feature 13\n",
    "                                feature13 = len(resolved_relations[(subject,obj)])\n",
    "\n",
    "                                print(subject,wdtitle,obj)\n",
    "                                \n",
    "                                for ip in pv:\n",
    "                                    if (feature1,feature2,feature3,feature4,feature5,\\\n",
    "                                                        feature6,feature7,feature8,feature9,feature10,\\\n",
    "                                                        feature11,feature12,feature13,ip,subject,wdtitle,obj,1) not in to_write:\n",
    "                                        to_write.add((feature1,feature2,feature3,feature4,feature5,\\\n",
    "                                                        feature6,feature7,feature8,feature9,feature10,\\\n",
    "                                                        feature11,feature12,feature13,ip,subject,wdtitle,obj))\n",
    "            \n",
    "            with open(file, 'a', newline='') as csvfile:\n",
    "                spamwriter = csv.writer(csvfile, delimiter=',')                                 \n",
    "                for twc in to_write:\n",
    "                    spamwriter.writerow(twc)\n",
    "                \n",
    "#                                 with open(file, 'a', newline='') as csvfile:\n",
    "#                                     spamwriter = csv.writer(csvfile, delimiter=',')\n",
    "#                                     spamwriter.writerow([feature1,feature2,feature3,feature4,feature5,\\\n",
    "#                                                         feature6,feature7,feature8,feature9,feature10,\\\n",
    "#                                                         feature11,feature12,feature13,pv[0],subject,wdtitle,obj])\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['Qaarsorsuaq Island', 'If... (Tohoshinki song)', 'Szabina Szlavikovics', 'Bdellomicrovirus', 'Bnei Akiva', 'Tlajomulco de Zúñiga', 'Ksar Hellal Congress', 'Wu-Chronicles', 'Gu Changwei', 'Ukai Thermal Power Station', 'Nha Sentimento', 'Sd.Kfz. 247', 'Cc65', 'Dconf', \"Mi'gmawei Mawiomi Secretariat\", 'Mhai', 'Db4o', 'Tjaart van der Walt', 'Vhembe District Municipality', \"Es'kia Mphahlele\", 'SmackDown (WWE brand)', 'Kkochi', 'Graakalbanen'])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nt1_2.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "defaultdict(list,\n",
       "            {('Japan', 'Japan'): ['http://www.wikidata.org/prop/direct/P17'],\n",
       "             ('Oricon', 'Japan'): ['http://www.wikidata.org/prop/direct/P17']})"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predicates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda env:anaconda3]",
   "language": "python",
   "name": "conda-env-anaconda3-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
