{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Load the normalized tables\n",
    "\n",
    "import pickle\n",
    "import sys\n",
    "sys.setrecursionlimit(10000)\n",
    "\n",
    "with open(\"normal_tables1_1.txt\", \"rb\") as fp:\n",
    "    nt1 = pickle.load(fp)\n",
    "    \n",
    "with open(\"normal_tables1_2.txt\", \"rb\") as fp:\n",
    "    nt2 = pickle.load(fp)\n",
    "    \n",
    "with open(\"normal_tables2_1.txt\", \"rb\") as fp:\n",
    "    nt3 = pickle.load(fp)\n",
    "    \n",
    "with open(\"normal_tables2_2.txt\", \"rb\") as fp:\n",
    "    nt4 = pickle.load(fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "186"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# number of articles containing tables with disambiguated entities (186)\n",
    "len(nt1) + len(nt2) + len(nt3) + len(nt4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# set up SPARQL endpoint for wikidata\n",
    "from SPARQLWrapper import SPARQLWrapper, JSON\n",
    "sparql = SPARQLWrapper(\"https://query.wikidata.org/sparql\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# resolve Wikidata entity from title\n",
    "\n",
    "from wikitables.client import Client\n",
    "\n",
    "client = Client(\"en\")\n",
    "\n",
    "def getWikidata(title):\n",
    "    return client.fetch_wikidata(title)\n",
    "\n",
    "def retrieveExtract(article):\n",
    "    return client.fetch_extract(article)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Features (as suggested by the authors (Emir Munoz & Aidan Hogan) of Wikitables Triples Paper)\n",
    "# =============\n",
    "# Table Features\n",
    "#     (-) 1 Number of rows\n",
    "#     (-) 2 Number of columns\n",
    "#     (-) 3 Total relations extracted\n",
    "# Column Features\n",
    "#     (+) 4 Potential relations\n",
    "#     (+) 5 Unique potential relations\n",
    "#     (+) 6 Entity relatedness (new)\n",
    "# Predicate Features\n",
    "#     (+) 7 Normalized unique subject count / Normalized unique object count\n",
    "# Cell Features\n",
    "#     (-) 8 Number of entities in subject cell\n",
    "#     (-) 9 Number of entities in object cell\n",
    "#     (-) 10 String length in subject cell\n",
    "#     (-) 11 String length in object cell\n",
    "# Predicate/Column Features\n",
    "#     (+) 12 Maximum between Jaro-Walker distance and dice coefficient\n",
    "#     (+) 13 Number of rows where the relation holds\n",
    "# Where (+) signifies a positive feature & (-) signifies a negative feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Predicate Features\n",
    "\n",
    "# subject and object must be prefixed with \"wd:\"\n",
    "# if the object is a value it must be double quoted\n",
    "#      getPredicates(\"wd:Q69\",\"'2830'\")\n",
    "# we get subject -> object, and its inverse object -> subject\n",
    "def getPredicates(subject,obj, number = False):\n",
    "    if number:\n",
    "        # we do a different query and return only the non-inverse\n",
    "        sparql.setQuery(\"\"\"SELECT * WHERE\n",
    "        {\n",
    "             %s ?p %s .\n",
    "             FILTER(STRSTARTS(str(?p), \"http://www.wikidata.org/prop/direct/\"))\n",
    "             SERVICE wikibase:label { bd:serviceParam wikibase:language \"en\" }\n",
    "        }\"\"\" % (subject, obj))\n",
    "        \n",
    "        sparql.setReturnFormat(JSON)\n",
    "        results = sparql.query().convert()\n",
    "        \n",
    "        predicates = list()\n",
    "\n",
    "        for row in results[\"results\"][\"bindings\"]:\n",
    "            if row[\"p\"][\"type\"] == \"uri\":\n",
    "                predicates.append(row[\"p\"][\"value\"])\n",
    "\n",
    "        return predicates\n",
    "    \n",
    "    sparql.setQuery(\"\"\"SELECT DISTINCT ?p1 ?p2\n",
    "    {\n",
    "         {%s ?p1 %s \n",
    "         FILTER(STRSTARTS(str(?p1), \"http://www.wikidata.org/prop/direct/\"))} \n",
    "         UNION {%s ?p2 %s\n",
    "         FILTER(STRSTARTS(str(?p2), \"http://www.wikidata.org/prop/direct/\"))}\n",
    "         SERVICE wikibase:label { bd:serviceParam wikibase:language \"en\" }\n",
    "    }\"\"\" % (subject, obj, obj, subject))\n",
    "    \n",
    "    sparql.setReturnFormat(JSON)\n",
    "    results = sparql.query().convert()\n",
    "    \n",
    "    predicates_so = list()\n",
    "    predicates_os = list()\n",
    "    \n",
    "    for row in results[\"results\"][\"bindings\"]:\n",
    "        try:\n",
    "            if row[\"p1\"][\"type\"] == \"uri\":\n",
    "                predicates_so.append(row[\"p1\"][\"value\"])\n",
    "            if row[\"p2\"][\"type\"] == \"uri\":\n",
    "                predicates_os.append(row[\"p2\"][\"value\"])\n",
    "        except:\n",
    "            pass\n",
    "\n",
    "    return predicates_so, predicates_os\n",
    "\n",
    "def checkTriple(subject, predicate, obj):\n",
    "    sparql.setQuery(\"\"\"ASK\n",
    "    {\n",
    "         %s %s %s .\n",
    "         SERVICE wikibase:label { bd:serviceParam wikibase:language \"en\" }\n",
    "    }\"\"\" % (subject, predicate, obj))\n",
    "    \n",
    "    sparql.setReturnFormat(JSON)\n",
    "    results = sparql.query().convert()\n",
    "    \n",
    "    return results[\"boolean\"]\n",
    "    \n",
    "# (+) 7 Normalized unique subject count / Normalized unique object count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Predicate/ Column Features\n",
    "# (+) 12 Max of dice coeffient and jaro-winkler distance\n",
    "from pyjarowinkler import distance\n",
    "\n",
    "def dice_coefficient(a,b):\n",
    "    if not len(a) or not len(b): return 0.0\n",
    "    \"\"\" quick case for true duplicates \"\"\"\n",
    "    if a == b: return 1.0\n",
    "    \"\"\" if a != b, and a or b are single chars, then they can't possibly match \"\"\"\n",
    "    if len(a) == 1 or len(b) == 1: return 0.0\n",
    "    \n",
    "    \"\"\" use python list comprehension, preferred over list.append() \"\"\"\n",
    "    a_bigram_list = [a[i:i+2] for i in range(len(a)-1)]\n",
    "    b_bigram_list = [b[i:i+2] for i in range(len(b)-1)]\n",
    "    \n",
    "    a_bigram_list.sort()\n",
    "    b_bigram_list.sort()\n",
    "    \n",
    "    # assignments to save function calls\n",
    "    lena = len(a_bigram_list)\n",
    "    lenb = len(b_bigram_list)\n",
    "    # initialize match counters\n",
    "    matches = i = j = 0\n",
    "    while (i < lena and j < lenb):\n",
    "        if a_bigram_list[i] == b_bigram_list[j]:\n",
    "            matches += 2\n",
    "            i += 1\n",
    "            j += 1\n",
    "        elif a_bigram_list[i] < b_bigram_list[j]:\n",
    "            i += 1\n",
    "        else:\n",
    "            j += 1\n",
    "    \n",
    "    score = float(matches)/float(lena + lenb)\n",
    "    return score\n",
    "\n",
    "def feature11(string1, string2):\n",
    "    return max(distance.get_jaro_distance(string1, string2, winkler=True, scaling=0.1),\\\n",
    "              dice_coefficient(string1, string2))\n",
    "\n",
    "# (+) 12 No of rows that contain the subject and object\n",
    "# def feature12(predicate):\n",
    "#     sparql.setQuery(\"\"\"SELECT * WHERE\n",
    "#     {\n",
    "#          ?s %s ?o .\n",
    "#          FILTER(STRSTARTS(str(?s), \"http://www.wikidata.org/entity/\"))\n",
    "#          FILTER(STRSTARTS(str(?o), \"http://www.wikidata.org/entity/\"))\n",
    "#          SERVICE wikibase:label { bd:serviceParam wikibase:language \"en\" }\n",
    "#     }\"\"\" % (predicate))\n",
    "    \n",
    "#     sparql.setReturnFormat(JSON)\n",
    "#     results = sparql.query().convert()\n",
    "\n",
    "#     return len(results[\"results\"][\"bindings\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# functions entitiy relatedness using API call\n",
    "import tagme\n",
    "\n",
    "with open(\"tagme\", 'r') as file:\n",
    "    token = file.readline().strip()\n",
    "    \n",
    "tagme.GCUBE_TOKEN = token\n",
    "\n",
    "def disambig(text, min_rho=None):\n",
    "    annotations = tagme.annotate(text)\n",
    "    a = dict()\n",
    "    for x in annotations.annotations:\n",
    "        if min_rho is None or x.score > min_rho:\n",
    "            a[str(x.mention)] = x.entity_title\n",
    "        \n",
    "    return a\n",
    "\n",
    "# Get relatedness between a pair of entities specified by title.\n",
    "# rels = tagme.relatedness_title((\"Barack Obama\", \"Italy\"))\n",
    "# print(\"Obama and italy have a semantic relation of\", rels.relatedness[0].rel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import csv\n",
    "\n",
    "# now that table has been recreated with only disambiguated entities\n",
    "# let the magic happen\n",
    "# extract all the triples and features\n",
    "\n",
    "def addTripleCSV(d, file, mode):\n",
    "    with open(file, mode, newline='') as csvfile:\n",
    "        fieldnames = ['id', 'subject', 'predicate','object']\n",
    "        writer = csv.DictWriter(csvfile, fieldnames=fieldnames)\n",
    "\n",
    "        if mode == 'w':\n",
    "            writer.writeheader()\n",
    "\n",
    "        writer.writerow(d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import simplemediawiki \n",
    "\n",
    "wiki = simplemediawiki.MediaWiki('https://www.wikidata.org/w/api.php')\n",
    "\n",
    "def findProperty(string, amount = None):\n",
    "    results = wiki.call({'action': 'wbsearchentities', 'search': string, 'type': 'property', 'language': 'en', 'limit': 10})\n",
    "\n",
    "    properties = list()\n",
    "    for i,x in enumerate(results['search']):\n",
    "        if amount and amount == i:\n",
    "            break\n",
    "        properties.append(x['id'])\n",
    "    \n",
    "    return properties"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# iterate through normalized and regular table\n",
    "\n",
    "import re\n",
    "def find_number(string):\n",
    "    return re.findall('\\d+', string)\n",
    "\n",
    "import itertools\n",
    "\n",
    "def get_pairs(l):\n",
    "    return list(itertools.combinations(l,2))\n",
    "\n",
    "def flatten(l):\n",
    "    return list(itertools.chain.from_iterable(l))\n",
    "\n",
    "def getCol(value, row):\n",
    "    for i,x in enumerate(row):\n",
    "        if isinstance(x, list):\n",
    "            for i2, v in enumerate(x):\n",
    "                if value == v: \n",
    "                    return (i,i2)\n",
    "        elif value == x:\n",
    "            return i\n",
    "        \n",
    "    # didn't make it\n",
    "    print(value, row)\n",
    "\n",
    "        \n",
    "def getSOPred(string1, string2, fnt, existingtriples, row = list(), section_entities = list(), pos = None):\n",
    "    e1 = string1.replace(\"'\",'\"')\n",
    "    e2 = string2.replace(\"'\",'\"')\n",
    "    n1 = False\n",
    "    n2  = False\n",
    "    if string1 not in fnt and string1 not in section_entities:\n",
    "        # not an actual entity; double quote\n",
    "        e1 = \"'\" + e1 + \"'\"\n",
    "        n1 = True\n",
    "    else:\n",
    "        # must catch exception because forgot to parse out deadlinks\n",
    "        try:\n",
    "            e1 = \"wd:\" + getWikidata(e1)\n",
    "        except:\n",
    "            e1 = \"'\" + e1 + \"'\"\n",
    "            n1 = True\n",
    "    if string2 not in fnt and string2 not in section_entities:\n",
    "        # not an actual entity; double quote\n",
    "        e2 = \"'\" + e2 + \"'\"\n",
    "        n2 = True\n",
    "    else:\n",
    "        # must catch exception because forgot to parse out deadlinks\n",
    "        try:\n",
    "            e2 = \"wd:\" + getWikidata(e2)\n",
    "        except:\n",
    "            e2 = \"'\" + e2 + \"'\"\n",
    "            n2 = True\n",
    "\n",
    "    if n1 and n2:\n",
    "        return\n",
    "\n",
    "    if not n1 and not n2:\n",
    "        pred1, pred2 = getPredicates(e1,e2)\n",
    "\n",
    "        if pred1:\n",
    "            # e1 ?p e2\n",
    "            if not pos:\n",
    "                s = (x, getCol(str(string1),row))\n",
    "            else:\n",
    "                s = pos\n",
    "            o = (x, getCol(str(string2), row))\n",
    "            for p1 in pred1:\n",
    "                existing_triples.append((s,p1,o))\n",
    "        if pred2:\n",
    "            # e2 ?p e1\n",
    "            if not pos:\n",
    "                o = (x, getCol(str(string1), row))\n",
    "            else:\n",
    "                o = pos\n",
    "            s = (x, getCol(str(string2), row))\n",
    "            for p1 in pred2:\n",
    "                existing_triples.append((s,p1,o))\n",
    "\n",
    "    elif n1 and not n2:\n",
    "        pred = getPredicates(e2,e1,True)\n",
    "        if pred:\n",
    "            # e2 ?p e1\n",
    "            if not pos:\n",
    "                o = (x, getCol(str(string1), row))\n",
    "            else:\n",
    "                o = pos\n",
    "            s = (x, getCol(str(string2), row))\n",
    "            for p1 in pred:\n",
    "                existing_triples.append((s,p1,o))\n",
    "\n",
    "    elif not n1 and n2:\n",
    "        pred = getPredicates(e1,e2,True)\n",
    "        if pred:\n",
    "            # e1 ?p e2\n",
    "            if not pos:\n",
    "                s = (x, getCol(str(string1), row))\n",
    "            else:\n",
    "                s = pos\n",
    "            o = (x, getCol(str(string2), row))\n",
    "            for p1 in pred:\n",
    "                existing_triples.append((s,p1,o))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "import itertools\n",
    "\n",
    "# helper functions to genreate candidate triples\n",
    "def checkPositionValid(pos, table):\n",
    "    try:\n",
    "        x = table[pos[0]][pos[1][0]][pos[1][1]]\n",
    "        return True\n",
    "    except:\n",
    "        return False\n",
    "\n",
    "def generateAllTuples(t, table):\n",
    "    s = set()\n",
    "    if t == (None, 'article') or t[1] == 'sub':\n",
    "        s.add((None,t))\n",
    "        return s\n",
    "    for i in range(1,len(table)):\n",
    "        pos = (i, t)\n",
    "        if checkPositionValid(pos, table):\n",
    "            s.add(pos)\n",
    "    return s\n",
    "\n",
    "def getElementsPosition(t):\n",
    "    s = set()\n",
    "    for x in t:\n",
    "        s.add(x[1])\n",
    "    return s\n",
    "\n",
    "\n",
    "# triple candidate generation\n",
    "def generateCandidates(xextracted,xextractedtable):\n",
    "    scount = defaultdict(int)\n",
    "    ocount = defaultdict(int)\n",
    "    scountset_o = defaultdict(set)\n",
    "    ocountset_s = defaultdict(set)\n",
    "\n",
    "    candidate_triples = set()\n",
    "\n",
    "    # count the occurences of subject and object if > threshold then suggest candidate triples & extract features\n",
    "    for t in set(xextracted):\n",
    "        scount[(t[0][1], t[1].split('/')[-1])]+=1\n",
    "        scountset_o[(t[0][1], t[1].split('/')[-1])].add(t[2])\n",
    "        ocount[(t[1].split('/')[-1], t[2][1])]+=1\n",
    "        ocountset_s[(t[1].split('/')[-1], t[2][1])].add(t[0])\n",
    "\n",
    "    PROPERTY_STRING = \"http://www.wikidata.org/prop/direct/\"\n",
    "\n",
    "    # do some set theory to find the candidate triple\n",
    "    # generate all possible sets and then take away the current sets\n",
    "\n",
    "    for k,v in scount.items():\n",
    "        if (v / (len(xextractedtable) - 1)) > 0.5:\n",
    "            spos = generateAllTuples(k[0], xextractedtable)\n",
    "            opos = getElementsPosition(scountset_o[k])\n",
    "            allo = set()\n",
    "            for x in opos:\n",
    "                allo = allo.union(generateAllTuples(x, xextractedtable))\n",
    "\n",
    "            # generate cartesian product between spos and allo and insert predicate into the middle\n",
    "            combos = list(itertools.product(spos, allo))\n",
    "\n",
    "            #print(combos)\n",
    "\n",
    "            # add to list of candidate triples\n",
    "            for c in combos:\n",
    "                if c[0][0] == c[1][0] or c[0][0] == None or c[1][0] == None:\n",
    "                    candidate_triples.add((c[0], PROPERTY_STRING + k[1], c[1]))\n",
    "\n",
    "\n",
    "\n",
    "    # this might do the same thing, but haven't proved it yet\n",
    "    for k,v in ocount.items():\n",
    "        if (v / (len(xextractedtable) - 1)) > 0.5:\n",
    "            opos = generateAllTuples(k[1], xextractedtable)\n",
    "            spos = getElementsPosition(ocountset_s[k])\n",
    "            alls = set()\n",
    "\n",
    "            for x in spos:\n",
    "                alls = alls.union(generateAllTuples(x, xextractedtable))\n",
    "\n",
    "            combos = list(itertools.product(alls, opos))\n",
    "\n",
    "            for c in combos:\n",
    "                if c[0][0] == c[1][0] or c[0][0] == None or c[1][0] == None:\n",
    "                    candidate_triples.add((c[0], PROPERTY_STRING + k[0], c[1]))     \n",
    "\n",
    "    candidate_triples = candidate_triples - set(xextracted)        \n",
    "    return candidate_triples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Db4o\n",
      "[['Version'], ['Features']]\n",
      "[['Db4o', 'Software developer', 'CONFIG.SYS', 'ASP.NET', 'COM file', 'Directory (computing)', 'Hypertext Transfer Protocol'], ['Software release life cycle', 'Office Open XML', 'Java (programming language)']]\n",
      "[['Db4o', 'Record producer'], ['Software release life cycle', 'Office Open XML', 'Java (programming language)']]\n",
      "[['Db4o', 'Software developer', 'CONFIG.SYS', 'ASP.NET', 'COM file', 'Directory (computing)', 'Hypertext Transfer Protocol'], ['Db4o', 'Network transparency', 'Eclipse (software)', 'Java collections framework', 'Java (programming language)', 'Persistence (computer science)']]\n",
      "((1, (0, 0)), 'http://www.wikidata.org/prop/direct/P277', (1, (1, 2)))\n",
      "((2, (0, 0)), 'http://www.wikidata.org/prop/direct/P277', (2, (1, 2)))\n",
      "((3, (0, 0)), 'http://www.wikidata.org/prop/direct/P277', (3, (1, 4)))\n",
      "((3, (0, 0)), 'http://www.wikidata.org/prop/direct/P277', (3, (1, 4)))\n",
      "((3, (1, 2)), 'http://www.wikidata.org/prop/direct/P277', (3, (1, 4)))\n",
      "((3, (1, 2)), 'http://www.wikidata.org/prop/direct/P144', (3, (1, 4)))\n",
      "((None, (0, 'sub')), 'http://www.wikidata.org/prop/direct/P1269', (1, (1, 0)))\n",
      "((None, (None, 'article')), 'http://www.wikidata.org/prop/direct/P277', (1, (1, 2)))\n",
      "((None, (0, 'sub')), 'http://www.wikidata.org/prop/direct/P1269', (2, (1, 0)))\n",
      "((None, (None, 'article')), 'http://www.wikidata.org/prop/direct/P277', (2, (1, 2)))\n",
      "((None, (None, 'article')), 'http://www.wikidata.org/prop/direct/P277', (3, (1, 4)))\n"
     ]
    }
   ],
   "source": [
    "tables = nt1['Db4o']\n",
    "\n",
    "k = 'Db4o'\n",
    "\n",
    "# for k,v in tables.items():\n",
    "#     print(k)\n",
    "#     for k1,v1 in v.items():\n",
    "for v in range(1):\n",
    "    print(k)\n",
    "    for k1,v1 in tables.items():\n",
    "        # hit table\n",
    "        old_table = v1[\"old_table\"]\n",
    "        new_table = v1[\"new_table\"]\n",
    "        section_title = v1[\"section_title\"]\n",
    "\n",
    "        len_rows = len(old_table)\n",
    "        len_cols = len(old_table[0])\n",
    "\n",
    "        temp_table = [[0 for y in range(len_cols)] for x in range(len_rows)]\n",
    "\n",
    "    #     # look at relationships between header (predicate) and cell (subject or object)\n",
    "    #     # header case\n",
    "    #     # disambiguate header (get first 3 results from wikidata search of header string)\n",
    "    #     header_pred = dict()\n",
    "    #     for y in range(len_cols):\n",
    "    #         header_pred[str(y)] = findProperty(old_table[0][y], 3)\n",
    "\n",
    "        # populate temp table (copy of new table) with values from original table if it hasn't been disambiguated\n",
    "        existing_triples = list() # contains list of (pos,predicate, pos)\n",
    "        for x in range(len_rows):\n",
    "            for y in range(len_cols):\n",
    "                if not new_table[x][y]:\n",
    "                    # try to make the obj a number\n",
    "                    n = find_number(old_table[x][y])\n",
    "                    if n:\n",
    "                        temp_table[x][y] = [str(n[0])]\n",
    "                    else:\n",
    "                        temp_table[x][y] = [str(old_table[x][y])]\n",
    "                else:\n",
    "                    temp_table[x][y] = new_table[x][y]\n",
    "\n",
    "\n",
    "            fnt = [x1 for x1 in flatten(new_table[x]) if x1!='']\n",
    "            if x > 0:\n",
    "                pairs = get_pairs([x1 for x1 in flatten(temp_table[x]) if x1!=''])\n",
    "                for p in pairs:\n",
    "                    getSOPred(p[0], p[1], fnt, existing_triples, temp_table[x])\n",
    "\n",
    "        # look at relationships between article title and cells\n",
    "        # look at relationships betweeen section title (if able to be disambiguated) and cells\n",
    "\n",
    "        # first disambiguate section title by combining article title, summary and section title\n",
    "        section_entities = list()\n",
    "        d = disambig(k + retrieveExtract(k) + section_title, 0.1)\n",
    "        for original,entitytitle in d.items():\n",
    "            if original in section_title:\n",
    "                section_entities.append(entitytitle)    \n",
    "\n",
    "        # skip the header row\n",
    "        for x in range(1,len_rows):\n",
    "            cells = [x1 for x1 in flatten(temp_table[x]) if x1!='']\n",
    "            fnt = [x1 for x1 in flatten(new_table[x]) if x1!='']        \n",
    "            for c in cells:\n",
    "                # match each cell with the header and possibly subsection title\n",
    "                if section_entities:\n",
    "                    for ise, se in enumerate(section_entities):\n",
    "                        getSOPred(se, c, fnt, existing_triples, temp_table[x], section_entities, (None,(ise, \"sub\")))\n",
    "\n",
    "                getSOPred(k, c, fnt, existing_triples, row = temp_table[x], pos = (None, (None, \"article\")))\n",
    "  \n",
    "        # based on exisiting triples, suggest candidate triples\n",
    "        # if the existing predicate happens > 50% in the same indices we suggest it\n",
    "        xextractedtable = temp_table\n",
    "        xextracted = existing_triples\n",
    "        for x in range(len_rows):\n",
    "            print(temp_table[x])\n",
    "        for et in existing_triples:\n",
    "            print(et)"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda env:anaconda3]",
   "language": "python",
   "name": "conda-env-anaconda3-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
